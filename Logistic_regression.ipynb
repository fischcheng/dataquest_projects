{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use GPA and GRE number to predict the outcome of application\n",
    "* Use Logistic regression to predict categorical outcome\n",
    "* Specify a threshold (subjective): discrimination threshold\n",
    "* logistic function, which is a version of the linear function that is adapted for classification.\n",
    "* scikit-learn LogisticRegression class: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "admissions=pd.read_csv('admissions.csv')\n",
    "\n",
    "plt.scatter(admissions.gpa,admissions.admit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAoiItLs/wPTQXdeZsX25QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Logistic Function\n",
    "def logistic(x):\n",
    "    # np.exp(x) raises x to the exponential power, ie e^x. e ~= 2.71828\n",
    "    return np.exp(x)  / (1 + np.exp(x)) \n",
    "    \n",
    "# Generate 50 real values, evenly spaced, between -6 and 6.\n",
    "x = np.linspace(-6,6,50, dtype=float)\n",
    "\n",
    "# Transform each number in t using the logistic function.\n",
    "y = logistic(x)\n",
    "\n",
    "# Plot the resulting data.\n",
    "plt.plot(x, y)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model=LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the predict probability instead of the label\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "\n",
    "pred_probs=logistic_model.predict_proba(admissions[[\"gpa\"]])\n",
    "\n",
    "plt.scatter(admissions[\"gpa\"],pred_probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "\n",
    "fitted_labels=logistic_model.predict(admissions[[\"gpa\"]])\n",
    "print(fitted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating bindary classifiers:\n",
    "* Accuracy: what fractions of predicted-label are correct = #corrected prediction/#observation\n",
    "* Sensitivity: True Positive Rate (TPR) = True positive / (true+ & false-) >> How effective is this model at identifying positive outcomes?\n",
    "* Specifity: Specificity or True Negative Rate - The proportion of applicants that were correctly rejected >> oppositve of TPR >> TNR\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"admissions.csv\")\n",
    "model = LogisticRegression()\n",
    "model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "\n",
    "admissions[\"predicted_label\"]=model.predict(admissions[[\"gpa\"]])\n",
    "print(admissions[\"predicted_label\"].value_counts())\n",
    "\n",
    "print(admissions.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "matches = admissions[\"predicted_label\"] == admissions[\"actual_label\"]\n",
    "correct_predictions = admissions[matches]\n",
    "\n",
    "print(correct_predictions.head())\n",
    "accuracy = len(correct_predictions) / len(admissions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_1 = (admissions[\"predicted_label\"] == admissions[\"actual_label\"]) & (admissions[\"predicted_label\"] ==1)\n",
    "true_positives = admissions[matches_1]\n",
    "\n",
    "matches_2 = (admissions[\"predicted_label\"] == admissions[\"actual_label\"]) & (admissions[\"predicted_label\"] ==0)\n",
    "true_negatives = admissions[matches_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sensitivity:\n",
    "# From the previous screen\n",
    "true_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "\n",
    "# From the previous screen\n",
    "false_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 1)\n",
    "false_negatives = len(admissions[false_negative_filter])\n",
    "\n",
    "sensitivity=true_positives/(true_positives+false_negatives)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifity\n",
    "# From previous screens\n",
    "true_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "false_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 0)\n",
    "false_positives = len(admissions[false_positive_filter])\n",
    "\n",
    "false_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 1)\n",
    "false_negatives = len(admissions[false_negative_filter])\n",
    "true_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 0)\n",
    "true_negatives = len(admissions[true_negative_filter])  \n",
    "\n",
    "specificity=true_negatives/(false_positives+true_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "* Working on Car-mileage data from https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n",
    "* Create numerical representation from categorical columns\n",
    "* Two approaches (1) One-vs-ALL (2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cars = pd.read_csv(\"auto.csv\")\n",
    "unique_regions=cars['origin'].unique()\n",
    "print(unique_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy variables for cyclinders and year\n",
    "dummy_cylinders = pd.get_dummies(cars[\"cylinders\"], prefix=\"cyl\")\n",
    "cars = pd.concat([cars, dummy_cylinders], axis=1)\n",
    "\n",
    "dummy_years = pd.get_dummies(cars[\"year\"], prefix=\"year\")\n",
    "cars = pd.concat([cars, dummy_years], axis=1)\n",
    "\n",
    "cars.drop(['year','cylinders'],axis=1,inplace=True)\n",
    "cars.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unique_origins = cars[\"origin\"].unique()\n",
    "unique_origins.sort()\n",
    "\n",
    "models = {}\n",
    "\n",
    "for origin in unique_origins:\n",
    "    features = [c for c in train.columns if c.startswith(\"cyl\") or c.startswith(\"year\")]\n",
    "    X=train[features]\n",
    "    y=train['origin']==origin\n",
    "    logre=LogisticRegression()\n",
    "    logre.fit(train[features], train['origin'])\n",
    "    \n",
    "    models[origin]=logre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_probs = pd.DataFrame(columns=unique_origins)\n",
    "\n",
    "for origin in unique_origins:\n",
    "    X_test=test[features]\n",
    "    testing_probs[origin]=models[origin].predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_origins=testing_probs.idxmax(axis=1)\n",
    "print(predicted_origins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* In these instances, it is always safer to treat discrete values as categorical variables. (Treat Year as categorical)\n",
    "* So category 1 vs the other, and find the highest probability of which origin for all three columns\n",
    "* First thing in any DS problems: (1) Regression or (2) Classification\n",
    "* The danger of overfitting: bias vs variance\n",
    "* Bias describes error that results in bad assumptions about the learning algorithm. \n",
    "* Variance describes error that occurs because of the variability of a model's predicted values.\n",
    "* We want low bias and low variance >> always a tradeoff\n",
    "* Find the Optimum model complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance vs Bias tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_test(cols):\n",
    "    model=LinearRegression()\n",
    "    model.fit(filtered_cars[cols],filtered_cars['mpg'])\n",
    "    predictions=model.predict(filtered_cars[cols])\n",
    "    variance=np.var(predictions)\n",
    "    mse=mean_squared_error(predictions,filtered_cars['mpg'])\n",
    "    return(mse,variance)\n",
    "    \n",
    "cyl_mse,cyl_var=train_and_test(['cylinders'])\n",
    "weight_mse,weight_var=train_and_test(['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation for train_and_test, takes in a list of strings.\n",
    "def train_and_test(cols):\n",
    "    # Split into features & target.\n",
    "    features = filtered_cars[cols]\n",
    "    target = filtered_cars[\"mpg\"]\n",
    "    # Fit model.\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(features, target)\n",
    "    # Make predictions on training set.\n",
    "    predictions = lr.predict(features)\n",
    "    # Compute MSE and Variance.\n",
    "    mse = mean_squared_error(filtered_cars[\"mpg\"], predictions)\n",
    "    variance = np.var(predictions)\n",
    "    return(mse, variance)\n",
    "\n",
    "one_mse, one_var = train_and_test([\"cylinders\"])\n",
    "\n",
    "two_mse,two_var=train_and_test([\"cylinders\",\"displacement\"])\n",
    "three_mse,three_var=train_and_test([\"cylinders\",\"displacement\",\"horsepower\"])\n",
    "four_mse,four_var=train_and_test([\"cylinders\",\"displacement\",\"horsepower\",\"weight\"])\n",
    "five_mse, five_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def train_and_cross_val(cols):\n",
    "    features=filtered_cars[cols]\n",
    "    target=filtered_cars['mpg']\n",
    "    \n",
    "    variance_values=[]\n",
    "    mse_values=[]\n",
    "    \n",
    "    \n",
    "    kf=KFold(n_splits=10, random_state=3, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "        \n",
    "        model=LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        mse=mean_squared_error(predictions,y_test)\n",
    "        variance=np.var(predictions)\n",
    "        \n",
    "        mse_values.append(mse)\n",
    "        variance_values.append(variance)\n",
    "        \n",
    "        \n",
    "    avg_mse=np.mean(mse_values)\n",
    "    avg_var=np.mean(variance_values)\n",
    "    return(avg_mse, avg_var)\n",
    "\n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", \"displacement\"])\n",
    "three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've hidden the `train_and_cross_val` function to save space but you can still call the function!\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", \"displacement\"])\n",
    "three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n",
    "\n",
    "x=range(2,8)\n",
    "y1=[two_mse,three_mse,four_mse,five_mse,six_mse,seven_mse]\n",
    "y2=[two_var,three_var,four_var,five_var,six_var,seven_var]\n",
    "\n",
    "plt.scatter(x,y1,color='red')\n",
    "plt.scatter(x,y2,color='blue')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* A good way to detect if your model is overfitting is to compare the in-sample error and the out-of-sample error, or the training error with the test error.\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
