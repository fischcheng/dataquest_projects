{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter in out based on criteria:\n",
    "class_size=data[\"class_size\"]\n",
    "class_size=class_size[class_size['GRADE ']=='09-12']\n",
    "class_size=class_size[class_size['PROGRAM TYPE']=='GEN ED']\n",
    "print(class_size.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby DBN to get average class-size for each shcool code\n",
    "import numpy as np\n",
    "class_size=class_size.groupby('DBN').agg(np.mean)\n",
    "class_size.reset_index(inplace=True)\n",
    "data['class_size']=class_size\n",
    "print(data['class_size'].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most recent school year\n",
    "demo=data['demographics']\n",
    "data['demographics']=demo[demo['schoolyear']==20112012]\n",
    "print(data[\"demographics\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad=data['graduation']\n",
    "grad=grad[grad['Cohort']=='2006']\n",
    "grad=grad[grad['Demographic']=='Total Cohort']\n",
    "data['graduation']=grad\n",
    "\n",
    "print(data['graduation'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_2010=data['ap_2010']\n",
    "cols = ['AP Test Takers ', 'Total Exams Taken', 'Number of Exams with scores 3 4 or 5']\n",
    "for col in cols:\n",
    "    data['ap_2010'][col]=pd.to_numeric(ap_2010[col],errors=\"coerce\")\n",
    "print(data['ap_2010'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas merge function >>> SQL join, left, right, inner, outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = data[\"sat_results\"]\n",
    "combined=combined.merge(data['ap_2010'],on='DBN',how='left')\n",
    "combined=combined.merge(data['graduation'],on='DBN',how='left')\n",
    "print(combined.head(5))\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join to ensure data exist across all combined tables.\n",
    "combined=combined.merge(data['class_size'],on='DBN',how='inner')\n",
    "combined=combined.merge(data['demographics'],on='DBN',how='inner')\n",
    "combined=combined.merge(data['survey'],on='DBN',how='inner')\n",
    "combined=combined.merge(data['hs_directory'],on='DBN',how='inner')\n",
    "\n",
    "print(combined.head(5))\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.fillna(combined.mean())\n",
    "combined = combined.fillna(0)\n",
    "print(combined.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first2(string):\n",
    "    return string[:2]\n",
    "combined['school_dist']=combined['DBN'].apply(first2)\n",
    "print(combined['school_dist'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now have a clean data set we can analyze! We've done a lot in this mission. We've gone from having several messy sources to one clean, combined, data set that's ready for analysis.\n",
    "\n",
    "Along the way, we've learned about:\n",
    "\n",
    "* How to handle missing values \n",
    "* Different types of merges\n",
    "* How to condense data sets\n",
    "* How to compute averages across dataframes\n",
    "\n",
    "Data scientists rarely start out with tidy data sets, which makes cleaning and combining them one of the most critical skills any data professional can learn.\n",
    "In the next mission, we'll analyze our clean data to find correlations and create maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
