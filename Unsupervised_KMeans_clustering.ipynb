{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previously on\n",
    "* Supervised: regression and classification\n",
    "* Unsupervised: clustering, reduce dimensionality\n",
    "* Clustering is a key way to explore unknown data, and it's a very commonly used machine learning technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US senator data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which senators are more or less in the mainstream of their party\n",
    "\n",
    "import pandas as pd\n",
    "votes=pd.read_csv('114_congress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring\n",
    "print(votes.party.value_counts())\n",
    "votes.mean()\n",
    "\n",
    "# If the mean for a column is less than .5, more Senators voted against the bill, and vice versa if it's over .5. Print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance between senators using only vote columns\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "print(euclidean_distances(votes.iloc[0,3:].values.reshape(1, -1), votes.iloc[1,3:].values.reshape(1, -1)))\n",
    "\n",
    "distance=euclidean_distances(votes.iloc[0,3:], votes.iloc[2,3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* Because we aren't predicting anything, there's no risk of overfitting, so we'll train our model on the whole dataset.\n",
    "* The crosstab() method takes in two vectors or Pandas Series and computes how many times each unique value in the second vector occurs for each unique value in the first vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=2, random_state=1)\n",
    "senator_distances=kmeans_model.fit_transform(votes.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use crosstab: occurence of same values for two vectors\n",
    "labels=kmeans_model.labels_\n",
    "pd.crosstab(labels,votes['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "democratic_outliers=votes[(labels==1) & (votes[\"party\"] == \"D\")]\n",
    "print(democratic_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "plt.scatter(senator_distances[:,0],senator_distances[:,1],c=labels,lw=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "extremism=(senator_distances**3).sum(axis=1)\n",
    "votes['extremism']=extremism\n",
    "\n",
    "votes.sort_values('extremism',inplace=True,ascending=False)\n",
    "print(votes.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBA K-Means clustering >> recreate the K-Means clustering process\n",
    "3 steps:\n",
    "(1) Assign each point to a clustter (evaluate the distance to randomly assigned centers)\n",
    "(2) Re-evaluate the points in each clustter to find new centers\n",
    "(3) Iterate the above processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "nba = pd.read_csv(\"nba_2013.csv\")\n",
    "nba.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out point_guards\n",
    "point_guards=nba[nba['pos']=='PG'].copy()\n",
    "\n",
    "# Create a new feature points per game\n",
    "point_guards['ppg'] = point_guards['pts'] / point_guards['g']\n",
    "\n",
    "# Sanity check, make sure ppg = pts/g\n",
    "point_guards[['pts', 'g', 'ppg']].head(5)\n",
    "\n",
    "# Assist-turnover ratio\n",
    "point_guards = point_guards[point_guards['tov'] != 0]\n",
    "point_guards['atr']=point_guards['ast']/point_guards['tov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(point_guards['ppg'], point_guards['atr'], c='y')\n",
    "plt.title(\"Point Guards\")\n",
    "plt.xlabel('Points Per Game', fontsize=13)\n",
    "plt.ylabel('Assist Turnover Ratio', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "# Use numpy's random function to generate a list, length: num_clusters, of indices\n",
    "random_initial_points = np.random.choice(point_guards.index, size=num_clusters)\n",
    "# Use the random indices to create the centroids\n",
    "centroids = point_guards.loc[random_initial_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the five randomly selected centroids\n",
    "plt.scatter(point_guards['ppg'], point_guards['atr'], c='yellow')\n",
    "plt.scatter(centroids['ppg'], centroids['atr'], c='red')\n",
    "plt.title(\"Centroids\")\n",
    "plt.xlabel('Points Per Game', fontsize=13)\n",
    "plt.ylabel('Assist Turnover Ratio', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroids_to_dict(centroids):\n",
    "    dictionary = dict()\n",
    "    # iterating counter we use to generate a cluster_id\n",
    "    counter = 0\n",
    "\n",
    "    # iterate a pandas data frame row-wise using .iterrows()\n",
    "    for index, row in centroids.iterrows():\n",
    "        coordinates = [row['ppg'], row['atr']]\n",
    "        dictionary[counter] = coordinates\n",
    "        counter += 1\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "centroids_dict = centroids_to_dict(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Euclidean distance\n",
    "import math\n",
    "\n",
    "def calculate_distance(centroid, player_values):\n",
    "    root_distance = 0\n",
    "    \n",
    "    for x in range(0, len(centroid)):\n",
    "        difference = centroid[x] - player_values[x]\n",
    "        squared_difference = difference**2\n",
    "        root_distance += squared_difference\n",
    "\n",
    "    euclid_distance = math.sqrt(root_distance)\n",
    "    return euclid_distance\n",
    "\n",
    "q = [5, 2]\n",
    "p = [3,1]\n",
    "\n",
    "# Sqrt(5) = ~2.24\n",
    "print(calculate_distance(q, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the function, `assign_to_cluster`\n",
    "# This creates the column, `cluster`, by applying assign_to_cluster row-by-row\n",
    "\n",
    "def assign_to_cluster(row):\n",
    "    \n",
    "    player_values=[row.ppg,row.atr]\n",
    "    dist=[]\n",
    "    for idx, centroid in centroids_dict.items():\n",
    "        euclidean_distance=calculate_distance(centroid,player_values)\n",
    "        dist.append(euclidean_distance)\n",
    "    cluster_id=np.argmin(dist)\n",
    "    return cluster_id\n",
    "\n",
    "point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing clusters\n",
    "def visualize_clusters(df, num_clusters):\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    for n in range(num_clusters):\n",
    "        clustered_df = df[df['cluster'] == n]\n",
    "        plt.scatter(clustered_df['ppg'], clustered_df['atr'], c=colors[n-1])\n",
    "        plt.xlabel('Points Per Game', fontsize=13)\n",
    "        plt.ylabel('Assist Turnover Ratio', fontsize=13)\n",
    "    plt.show()\n",
    "\n",
    "visualize_clusters(point_guards, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_centroids(df):\n",
    "    new_centroids_dict = dict()\n",
    "    # 0..1...2...3...4\n",
    "    for cluster_id in range(0, num_clusters):\n",
    "        # Finish the logic\n",
    "        partial=df[df['cluster']==cluster_id]\n",
    "        new_centroids_dict[cluster_id]=[partial['ppg'].mean(),partial['atr'].mean()]\n",
    "        \n",
    "        return new_centroids_dict\n",
    "\n",
    "centroids_dict = recalculate_centroids(point_guards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun assign to cluster based on new centroids:\n",
    "point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)\n",
    "visualize_clusters(point_guards, num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "To counteract these problems, the sklearn implementation of K-Means does some intelligent things like re-running the entire clustering process lots of times with random initial centroids so the final results are a little less biased on one passthrough's initial centroids.\n",
    "\n",
    "* For many machine learning algorithms it's important to scale, or normalize, the data before using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* PGA data, the negative correlation between accuracy and distance\n",
    "* Gradient descent is a general method that can be used to estimate coefficents of nearly any model, including linear models. At it's core, gradient descent minimizes the residuals in the estimated model by updating each coefficent based on it's gradient.\n",
    "* What is the cost function? For Linear regression $J(\\theta_{0},\\theta_{1})=\\frac{1}{2m}\\sum\\limits_{i=1}^m(h_{\\theta}(x_{i})-y_{i})^{2}$\n",
    "* The minimum of the cost function is the point where the model has the lowest error, hence the point where our parameters are optimized.\n",
    "* Gradient descent relies on finding the direction of the largest gradient where a gradient is the \"slope\" of a multivariable function. To find this gradient we can take the partial derivative in terms of each parameter in the cost function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# We can add a dimension to an array by using np.newaxis\n",
    "print(\"Shape of the series:\", pga['distance'].shape)\n",
    "print(\"Shape with newaxis:\", pga['distance'][:, np.newaxis].shape)\n",
    "\n",
    "# The X variable in LinearRegression.fit() must have 2 dimensions\n",
    "model=LinearRegression()\n",
    "model.fit(pga[['distance']],pga['accuracy'])\n",
    "\n",
    "theta1=model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost function of a single variable linear model\n",
    "def cost(theta0, theta1, x, y):\n",
    "    # Initialize cost\n",
    "    J = 0\n",
    "    # The number of observations\n",
    "    m = len(x)\n",
    "    # Loop through each observation\n",
    "    for i in range(m):\n",
    "        # Compute the hypothesis \n",
    "        h = theta1 * x[i] + theta0\n",
    "        # Add to cost\n",
    "        J += (h - y[i])**2\n",
    "    # Average and normalize cost\n",
    "    J /= (2*m)\n",
    "    return J\n",
    "\n",
    "# The cost for theta0=0 and theta1=1\n",
    "print(cost(0, 1, pga['distance'], pga['accuracy']))\n",
    "\n",
    "theta0 = 100\n",
    "theta1s = np.linspace(-3,2,100)\n",
    "costs=cost(theta0,theta1s,pga['distance'], pga['accuracy'])\n",
    "\n",
    "plt.plot(theta1s,costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Example of a Surface Plot using Matplotlib\n",
    "# Create x an y variables\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.linspace(-10,10,100)\n",
    "\n",
    "# We must create variables to represent each possible pair of points in x and y\n",
    "# ie. (-10, 10), (-10, -9.8), ... (0, 0), ... ,(10, 9.8), (10,9.8)\n",
    "# x and y need to be transformed to 100x100 matrices to represent these coordinates\n",
    "# np.meshgrid will build a coordinate matrices of x and y\n",
    "X, Y = np.meshgrid(x,y)\n",
    "print(X[:5,:5],\"\\n\",Y[:5,:5])\n",
    "\n",
    "# Compute a 3D parabola \n",
    "Z = X**2 + Y**2 \n",
    "\n",
    "# Open a figure to place the plot on\n",
    "fig = plt.figure()\n",
    "# Initialize 3D plot\n",
    "ax = fig.gca(projection='3d')\n",
    "# Plot the surface\n",
    "ax.plot_surface(X=X,Y=Y,Z=Z)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Use these for your exercise\n",
    "theta0s = np.linspace(-2,2,100)\n",
    "theta1s = np.linspace(-2,2, 100)\n",
    "COST = np.empty(shape=(100,100))\n",
    "\n",
    "# Meshgrid for paramaters \n",
    "T0S, T1S = np.meshgrid(theta0s, theta1s)\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        COST[i,j]=cost(T0S[0,i],T1S[j,0],pga['distance'], pga['accuracy'])\n",
    "        \n",
    "# 3d Plot\n",
    "fig2 = plt.figure()\n",
    "ax = fig2.gca(projection='3d')\n",
    "ax.plot_surface(X=T0S,Y=T1S,Z=COST)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_cost_theta1(theta0, theta1, x, y):\n",
    "    # Hypothesis\n",
    "    h = theta0 + theta1*x\n",
    "    # Hypothesis minus observed times x\n",
    "    diff = (h - y) * x\n",
    "    # Average to compute partial derivative\n",
    "    partial = diff.sum() / (x.shape[0])\n",
    "    return partial\n",
    "\n",
    "partial1 = partial_cost_theta1(0, 5, pga['distance'], pga['accuracy'])\n",
    "print(\"partial1 =\", partial1)\n",
    "\n",
    "\n",
    "def partial_cost_theta0(theta0, theta1, x, y):\n",
    "    # Hypothesis\n",
    "    h = theta0 + theta1*x\n",
    "    # Hypothesis minus observed times x\n",
    "    diff = h - y\n",
    "    # Average to compute partial derivative\n",
    "    partial0 = diff.sum() / (x.shape[0])\n",
    "    return partial0\n",
    "\n",
    "partial0 = partial_cost_theta0(1, 1, pga['distance'], pga['accuracy'])\n",
    "print(\"partial0 =\", partial0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* We measure convergence by finding the difference between the previous iterations cost versus the current cost.\n",
    "* Formula $\\theta_{1}:=\\theta_{1}-\\alpha \\cdot \\frac{\\partial{J}(\\theta_{0},\\theta_{1})}{\\partial \\theta_{1}}$ ; $\\theta_{0}:=\\theta_{0}-\\alpha \\cdot \\frac{\\partial{J}(\\theta_{0},\\theta_{1})}{\\partial \\theta_{0}}$\n",
    "* $\\alpha$ is the learning rate: This value is set by the user and controls how fast the algorithm will converge by changing the parameters by some percentage of the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is our feature vector -- distance\n",
    "# y is our target variable -- accuracy\n",
    "# alpha is the learning rate\n",
    "# theta0 is the initial theta0 \n",
    "# theta1 is the initial theta1\n",
    "def gradient_descent(x, y, alpha=0.1, theta0=0, theta1=0):\n",
    "    max_epochs = 1000 # Maximum number of iterations\n",
    "    counter = 0      # Initialize a counter\n",
    "    c = cost(theta0, theta1, pga['distance'], pga['accuracy'])  ## Initial cost\n",
    "    costs = [c]     # Lets store each update\n",
    "    # Set a convergence threshold to find where the cost function in minimized\n",
    "    # When the difference between the previous cost and current cost \n",
    "    #        is less than this value we will say the parameters converged\n",
    "    convergence_thres = 0.000001  \n",
    "    cprev = c + 10   \n",
    "    theta0s = [theta0]\n",
    "    theta1s = [theta1]\n",
    "\n",
    "    # When the costs converge or we hit a large number of iterations will we stop updating\n",
    "    while (np.abs(cprev - c) > convergence_thres) and (counter < max_epochs):\n",
    "        cprev = c\n",
    "        # Alpha times the partial derivative is our updated\n",
    "        update0 = alpha * partial_cost_theta0(theta0, theta1, x, y)\n",
    "        update1 = alpha * partial_cost_theta1(theta0, theta1, x, y)\n",
    "\n",
    "        # Update theta0 and theta1 at the same time\n",
    "        # We want to compute the slopes at the same set of hypothesised parameters\n",
    "        #             so we update after finding the partial derivatives\n",
    "        theta0 -= update0\n",
    "        theta1 -= update1\n",
    "        \n",
    "        # Store thetas\n",
    "        theta0s.append(theta0)\n",
    "        theta1s.append(theta1)\n",
    "        \n",
    "        # Compute the new cost\n",
    "        c = cost(theta0, theta1, pga['distance'], pga['accuracy'])\n",
    "\n",
    "        # Store updates\n",
    "        costs.append(c)\n",
    "        counter += 1   # Count\n",
    "\n",
    "    return {'theta0': theta0, 'theta1': theta1, \"costs\": costs}\n",
    "\n",
    "print(\"Theta1 =\", gradient_descent(pga['distance'], pga['accuracy'])['theta1'])\n",
    "\n",
    "descend = gradient_descent(pga['distance'], pga['accuracy'], alpha=.01)\n",
    "\n",
    "plt.scatter(range(len(descend[\"costs\"])), descend[\"costs\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
