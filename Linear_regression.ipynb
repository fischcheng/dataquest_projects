{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('AmesHousing.txt', sep=\"\\t\")\n",
    "train=data.iloc[:1460]\n",
    "test=data.iloc[1460:]\n",
    "\n",
    "data.info()\n",
    "target = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting criteria:\n",
    "* Residual Sum Of Squares\n",
    "\n",
    "To find the optimal parameters for a linear regression model, we want to optimize the model's residual sum of squares (or RSS). If you call, residual (often referred to as errors) describes the difference between the predicted values for the target column () and the true values ():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Scikit learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg=LinearRegression()\n",
    "#reg.fit(train[['Garage Area']],train[\"SalePrice\"])\n",
    "reg.fit(train[['Gr Liv Area']],train[\"SalePrice\"])\n",
    "#reg.fit(train[['Overall Cond']],train[\"SalePrice\"])\n",
    "\n",
    "a1= reg.coef_# first coefficient, slope\n",
    "a0= reg.intercept_# intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test RMSE\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[['Gr Liv Area']], train['SalePrice'])\n",
    "\n",
    "predictions=lr.predict(train[['Gr Liv Area']])\n",
    "train_rmse=mean_squared_error(train['SalePrice'],predictions)**.5\n",
    "\n",
    "predictions=lr.predict(test[['Gr Liv Area']])\n",
    "test_rmse=mean_squared_error(test['SalePrice'],predictions)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### More than one feature:\n",
    "cols = ['Overall Cond', 'Gr Liv Area']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[cols], train['SalePrice'])\n",
    "\n",
    "predictions=lr.predict(train[cols])\n",
    "train_rmse_2=mean_squared_error(train['SalePrice'],predictions)**.5\n",
    "\n",
    "predictions=lr.predict(test[cols])\n",
    "test_rmse_2=mean_squared_error(test['SalePrice'],predictions)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "numerical_train = train.select_dtypes(include=['int', 'float'])\n",
    "numerical_train=numerical_train.drop(['PID','Year Built','Year Remod/Add','Garage Yr Blt','Mo Sold','Yr Sold'],axis=1)\n",
    "null_series=numerical_train.isnull().sum()\n",
    "full_cols_series=null_series[null_series==0]\n",
    "print(full_cols_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train[full_cols_series.index]\n",
    "correlation=train_subset.corr()\n",
    "sorted_corrs=correlation.SalePrice.abs().sort_values()\n",
    "print(sorted_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "strong_corrs=sorted_corrs[sorted_corrs>0.3]\n",
    "corrmat = train_subset[strong_corrs.index].corr()\n",
    "sns.heatmap(corrmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    "features = final_corr_cols.drop(['SalePrice']).index\n",
    "target = 'SalePrice'\n",
    "clean_test=test[features].dropna()\n",
    "\n",
    "lr=LinearRegression()\n",
    "lr.fit(train[features],train['SalePrice'])\n",
    "\n",
    "train_predictions = lr.predict(train[features])\n",
    "test_predictions = lr.predict(clean_test[features])\n",
    "\n",
    "train_mse=mean_squared_error(train_predictions,train[target])\n",
    "test_mse=mean_squared_error(test_predictions,clean_test[target])\n",
    "\n",
    "train_rmse=np.sqrt(train_mse)\n",
    "test_rmse=np.sqrt(test_mse)\n",
    "\n",
    "print(train_rmse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled=train[features].apply(lambda x: (x-x.min())/(x.max()-x.min()))\n",
    "print(rescaled)\n",
    "\n",
    "sorted_vars=rescaled.var().sort_values()\n",
    "print(sorted_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=features.drop('Open Porch SF')\n",
    "clean_test = test[final_corr_cols.index].dropna()\n",
    "\n",
    "lr=LinearRegression()\n",
    "lr.fit(train[features],train['SalePrice'])\n",
    "\n",
    "train_predictions=lr.predict(train[features])\n",
    "train_mse=mean_squared_error(train_predictions,train['SalePrice'])\n",
    "\n",
    "test_predictions=lr.predict(clean_test[features])\n",
    "test_mse=mean_squared_error(test_predictions,clean_test['SalePrice'])                           \n",
    "train_rmse_2=np.sqrt(train_mse)\n",
    "test_rmse_2=np.sqrt(test_mse)\n",
    "                             \n",
    "print(train_rmse_2)\n",
    "print(test_rmse_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* Feature selection based on correlation\n",
    "* Co-linearity between features\n",
    "* Use correlation matrix heat-map\n",
    "* Rescaling\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "#### Select initial values for the parameter: \n",
    "* repeat until convergence (usually implemented with a max number of iterations):\n",
    "* calculate the error (MSE) of model that uses current parameter value: \n",
    "* calculate the derivative of the error (MSE) at the current parameter value: \n",
    "* update the parameter value by subtracting the derivative times a constant (, called the learning rate): \n",
    "\n",
    "\n",
    "#### For every iteration of gradient descent:\n",
    "* this derivative is computed using the current  value\n",
    "* the derivative is multiplied by the learning rate (): \n",
    "* the result is subtracted from the current parameter value and assigned as the new parameter value: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(a1, xi_list, yi_list):\n",
    "    \n",
    "    result=2/len(xi_list)*(xi_list*(a1*xi_list-yi_list)).sum()\n",
    "    return result\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
    "    a1_list = [a1_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        deriv = derivative(a1, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha*deriv\n",
    "        a1_list.append(a1_new)\n",
    "    return(a1_list)\n",
    "\n",
    "# Uncomment when ready.\n",
    "param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
    "final_param = param_iterations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two parameters (a0 and a1)\n",
    "def a1_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def a0_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += (a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
    "    a1_list = [a1_initial]\n",
    "    a0_list = [a0_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        a0 = a0_list[i]\n",
    "        \n",
    "        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)\n",
    "        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)\n",
    "        \n",
    "        a1_new = a1 - alpha*a1_deriv\n",
    "        a0_new = a0 - alpha*a0_deriv\n",
    "        \n",
    "        a1_list.append(a1_new)\n",
    "        a0_list.append(a0_new)\n",
    "    return(a0_list, a1_list)\n",
    "\n",
    "# Uncomment when ready.\n",
    "a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Linear Squares (OLS)\n",
    "* Use linear algebra instead of gradient descent to find the findal coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
    "       'Gr Liv Area', 'Overall Qual']\n",
    "\n",
    "X=train[features]\n",
    "y=train['SalePrice']\n",
    "\n",
    "first_term = np.linalg.inv(\n",
    "        np.dot(np.transpose(X), X)\n",
    ")\n",
    "second_term = np.dot(\n",
    "        np.transpose(X),\n",
    "        y\n",
    "    )\n",
    "a = np.dot(first_term, second_term)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Derivation of OLS fomula: https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Transforming features: >> feature Engineering\n",
    "* transform categorical colums to the correct type: \n",
    "* dummy encoding: separate categorical columns into multiple column with dummy numerics\n",
    "* Having some domain knowledge can help with determining an acceptable cutoff value. >> and feature engineering\n",
    "* Two ways to deal with missing data: (1) Remove (2) Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "train_null_counts = train.isnull().sum()\n",
    "print(train_null_counts)\n",
    "\n",
    "df_no_mv=train[train_null_counts[train_null_counts==0].index]\n",
    "df_no_mv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = df_no_mv.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in text_cols:\n",
    "    #print(col+\":\", len(train[col].unique()))\n",
    "    train[col]=train[col].astype('category')\n",
    "    \n",
    "train['Utilities'].cat.codes\n",
    "print(train['Utilities'].cat.codes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = pd.DataFrame()\n",
    "for col in text_cols:\n",
    "    col_dummies = pd.get_dummies(train[col])\n",
    "    train = pd.concat([train, col_dummies], axis=1)\n",
    "    del train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['years_until_remod']=train['Year Remod/Add']-train['Year Built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "train_null_counts = train.isnull().sum()\n",
    "df_missing_values=train[train_null_counts[(train_null_counts>0) & (train_null_counts<584)].index]\n",
    "\n",
    "print(df_missing_values.isnull().sum())\n",
    "print(df_missing_values.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with column mean\n",
    "float_cols = df_missing_values.select_dtypes(include=['float'])\n",
    "\n",
    "float_cols = float_cols.fillna(float_cols.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "* As we mentioned earlier, succeeding in predictive modeling (and competitions like Kaggle) is highly dependent on the quality of features the model has."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
