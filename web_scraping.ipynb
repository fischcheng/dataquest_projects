{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data using API\n",
    "* No need to construct the full dataset\n",
    "* ready-prepared .csv files are not always available.\n",
    "* Use request library to ask remote server to return .json files (JavaScript Object Notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current position of the International Space Station through OPEN-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Make a get request to get the latest position of the ISS from the OpenNotify API.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "\n",
    "status_code=response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some status_code\n",
    "* 200 - Everything went okay, and the server returned a result (if any).\n",
    "* 301 - The server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint's name has changed.\n",
    "* 401 - The server thinks you're not authenticated. This happens when you don't send the right credentials to access an API (we'll talk about this in a later mission).\n",
    "* 400 - The server thinks you made a bad request. This can happen when you don't send the information the API requires to process your request, among other things.\n",
    "* 403 - The resource you're trying to access is forbidden; you don't have the right permissions to see it.\n",
    "* 404 - The server didn't find the resource you tried to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/iss-pass\")\n",
    "status_code=response.status_code\n",
    "status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"message\": \"success\", \\n  \"request\": {\\n    \"altitude\": 100, \\n    \"datetime\": 1533099994, \\n    \"latitude\": 37.78, \\n    \"longitude\": -122.41, \\n    \"passes\": 5\\n  }, \\n  \"response\": [\\n    {\\n      \"duration\": 570, \\n      \"risetime\": 1533101652\\n    }, \\n    {\\n      \"duration\": 643, \\n      \"risetime\": 1533107429\\n    }, \\n    {\\n      \"duration\": 465, \\n      \"risetime\": 1533113276\\n    }, \\n    {\\n      \"duration\": 437, \\n      \"risetime\": 1533161693\\n    }, \\n    {\\n      \"duration\": 639, \\n      \"risetime\": 1533167348\\n    }\\n  ]\\n}\\n'\n",
      "b'{\\n  \"message\": \"success\", \\n  \"request\": {\\n    \"altitude\": 100, \\n    \"datetime\": 1533099994, \\n    \"latitude\": 37.78, \\n    \"longitude\": -122.41, \\n    \"passes\": 5\\n  }, \\n  \"response\": [\\n    {\\n      \"duration\": 570, \\n      \"risetime\": 1533101652\\n    }, \\n    {\\n      \"duration\": 643, \\n      \"risetime\": 1533107429\\n    }, \\n    {\\n      \"duration\": 465, \\n      \"risetime\": 1533113276\\n    }, \\n    {\\n      \"duration\": 437, \\n      \"risetime\": 1533161693\\n    }, \\n    {\\n      \"duration\": 639, \\n      \"risetime\": 1533167348\\n    }\\n  ]\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameters we want to pass to the API.\n",
    "# This is the latitude and longitude of New York City.\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "\n",
    "# Make a get request with the parameters.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "\n",
    "# Print the content of the response (the data the server returned)\n",
    "print(response.content)\n",
    "content=response.content\n",
    "\n",
    "# This gets the same data as the command above\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json?lat=37.78&lon=-122.41\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything .get is now a string (essentially JSON)\n",
    "we need to use json library to dump or load the string back to usable forms in python (list, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Make a list of fast food chains.\n",
    "best_food_chains = [\"Taco Bell\", \"Shake Shack\", \"Chipotle\"]\n",
    "print(type(best_food_chains))\n",
    "\n",
    "\n",
    "# Use json.dumps to convert best_food_chains to a string.\n",
    "best_food_chains_string = json.dumps(best_food_chains)\n",
    "print(type(best_food_chains_string))\n",
    "\n",
    "# Convert best_food_chains_string back to a list.\n",
    "print(type(json.loads(best_food_chains_string)))\n",
    "\n",
    "# Make a dictionary\n",
    "fast_food_franchise = {\n",
    "    \"Subway\": 24722,\n",
    "    \"McDonalds\": 14098,\n",
    "    \"Starbucks\": 10821,\n",
    "    \"Pizza Hut\": 7600\n",
    "}\n",
    "\n",
    "# We can also dump a dictionary to a string and load it.\n",
    "fast_food_franchise_string = json.dumps(fast_food_franchise)\n",
    "print(type(fast_food_franchise_string))\n",
    "\n",
    "fast_food_franchise_2=json.loads(fast_food_franchise_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"message\": \"success\", \"people\": [{\"craft\": \"ISS\", \"name\": \"Oleg Artemyev\"}, {\"craft\": \"ISS\", \"name\": \"Andrew Feustel\"}, {\"craft\": \"ISS\", \"name\": \"Richard Arnold\"}, {\"craft\": \"ISS\", \"name\": \"Sergey Prokopyev\"}, {\"craft\": \"ISS\", \"name\": \"Alexander Gerst\"}, {\"craft\": \"ISS\", \"name\": \"Serena Aunon-Chancellor\"}], \"number\": 6}'\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "\n",
    "in_space_count=response.content\n",
    "print(in_space_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "response = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "cont=response.json()\n",
    "in_space_count=cont['number']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use token and Github API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of headers containing our Authorization header.\n",
    "headers = {\"Authorization\": \"token 1f36137fbbe1602f779300dad26e4c1b7fbab631\"}\n",
    "\n",
    "# Make a GET request to the GitHub API with our headers.\n",
    "# This API endpoint will give us details about Vik Paruchuri.\n",
    "response = requests.get(\"https://api.github.com/users/VikParuchuri/orgs\", headers=headers)\n",
    "\n",
    "orgs=response.json()\n",
    "\n",
    "# Print the content of the response.  As you can see, this token corresponds to the account of Vik Paruchuri.\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've loaded headers in.\n",
    "headers = {\"Authorization\": \"token 1f36137fbbe1602f779300dad26e4c1b7fbab631\"}\n",
    "\n",
    "response=requests.get(\"https://api.github.com/users/torvalds\",headers=headers)\n",
    "torvalds=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here.\n",
    "response=requests.get(\"https://api.github.com/repos/octocat/Hello-World\",headers=headers)\n",
    "hello_world=response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pagination\n",
    "This isn't a great user experience, so it's typical for API providers to implement pagination. This means that the API provider will only return a certain number of records per page. You can specify the page number that you want to access. To access all of the pages, you'll need to write a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"per_page\": 50, \"page\": 2}\n",
    "response = requests.get(\"https://api.github.com/users/VikParuchuri/starred\", headers=headers, params=params)\n",
    "#page1_repos = response.json()\n",
    "\n",
    "\n",
    "page2_repos= response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here.\n",
    "response=requests.get(\"https://api.github.com/user\",headers=headers)\n",
    "user=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data we'll pass into the API endpoint.  While this endpoint only requires the \"name\" key, there are other optional keys.\n",
    "payload = {\"name\": \"learning-about-apis\"}\n",
    "\n",
    "# We need to pass in our authentication headers!\n",
    "response = requests.post(\"https://api.github.com/user/repos\", json=payload, headers=headers)\n",
    "status=response.status_code\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch or Put\n",
    "payload = {\"description\": \"Learning about requests!\", \"name\": \"learning-about-apis\"}\n",
    "response = requests.patch(\"https://api.github.com/repos/VikParuchuri/learning-about-apis\", json=payload, headers=headers)\n",
    "status=response.status_code\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete\n",
    "response = requests.delete(\"https://api.github.com/repos/VikParuchuri/learning-about-apis\", headers=headers)\n",
    "status=response.status_code\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* not all APIs accept all options under requests\n",
    "* Most common, request.GET > to retrieve data \n",
    "* Github API accepts requests.POST (to create new repo for https://api.github.com/user/repos endpoint) > return 201 if success\n",
    "* requests.PATCH (attributes) and request.PUT (whole object) to update existing items > return 200 if success\n",
    "* requests.DELETE > return 204 if success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit API challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "params = {\"t\": \"day\"}\n",
    "response=requests.get(\"https://oauth.reddit.com/r/python/top\",headers=headers,params=params)\n",
    "python_top=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most upvoted post in the past day:\n",
    "python_top_articles=python_top[\"data\"][\"children\"]\n",
    "most_upvoted = \"\"\n",
    "most_upvotes = 0\n",
    "\n",
    "for article in python_top_articles:\n",
    "    ar = article[\"data\"]\n",
    "    if ar[\"ups\"] >= most_upvotes:\n",
    "        most_upvoted = ar[\"id\"]\n",
    "        most_upvotes = ar[\"ups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "\n",
    "response=requests.get(\"https://oauth.reddit.com/r/python/comments/4b7w9u\",headers=headers)\n",
    "comments=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list=comments[1]['data']['children']\n",
    "\n",
    "most_upvoted_comment = \"\"\n",
    "most_upvotes = 0\n",
    "\n",
    "for article in comments_list:\n",
    "    ar = article[\"data\"]\n",
    "    if ar[\"ups\"] >= most_upvotes:\n",
    "        most_upvoted_comment = ar[\"id\"]\n",
    "        most_upvotes = ar[\"ups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "\n",
    "payload={'dir':1,'id':'d16y4ry'}\n",
    "response=requests.post('https://oauth.reddit.com/api/vote',headers=headers,json=payload)\n",
    "\n",
    "status=response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A simple example page\n"
     ]
    }
   ],
   "source": [
    "# Request to the html file\n",
    "import requests\n",
    "response=requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "content=response.content\n",
    "\n",
    "# Use BeautifulSoup to parse the html \n",
    "from bs4 import BeautifulSoup\n",
    "# Initialize the parser, and pass in the content we grabbed earlier.\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the body tag from the document.\n",
    "# Since we passed in the top level of the document to the parser, we need to pick a branch off of the root.\n",
    "# With BeautifulSoup, we can access branches by using tag types as attributes.\n",
    "body = parser.body\n",
    "header=parser.head\n",
    "# Get the p tag from the body.\n",
    "p = body.p\n",
    "t = header.title\n",
    "# Print the text inside the p tag.\n",
    "# Text is a property that gets the inside text of a tag.\n",
    "title_text=t.text\n",
    "print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get a list of all occurrences of the body tag in the element.\n",
    "body = parser.find_all(\"body\")\n",
    "head= parser.find_all(\"head\")\n",
    "\n",
    "# Get the paragraph tag.\n",
    "p = body[0].find_all(\"p\")\n",
    "t = head[0].find_all(\"title\")\n",
    "# Get the text.\n",
    "title_text=t[0].text\n",
    "print(p[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# Get the page content and set up a new parser.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_ids.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Pass in the ID attribute to only get the element with that specific ID.\n",
    "first_paragraph = parser.find_all(\"p\", id=\"first\")[0]\n",
    "print(first_paragraph.text)\n",
    "\n",
    "second_paragraph = parser.find_all(\"p\", id=\"second\")[0]\n",
    "second_paragraph_text = second_paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# Get the website that contains classes.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the first inner paragraph.\n",
    "# Find all the paragraph tags with the class inner-text.\n",
    "# Then, take the first element in that list.\n",
    "first_inner_paragraph = parser.find_all(\"p\", class_=\"inner-text\")[0]\n",
    "print(first_inner_paragraph.text)\n",
    "\n",
    "\n",
    "second_inner_paragraph_text=parser.find_all(\"p\", class_=\"inner-text\")[1].text\n",
    "\n",
    "first_outer_paragraph_text=parser.find_all(\"p\", class_=\"outer-text\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# We can use BeautifulSoup's .select method to work with CSS selectors. \n",
    "# Here's the HTML we'll be working with on this screen:\n",
    "# Get the website that contains classes and IDs.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Select all of the elements that have the first-item class.\n",
    "first_items = parser.select(\".first-item\")\n",
    "\n",
    "first_outer_text = parser.select(\".outer-text\")[0].text\n",
    "second_text = parser.select(\"#second\")[0].text\n",
    "\n",
    "\n",
    "# Print the text of the first paragraph (the first element with the first-item class).\n",
    "print(first_items[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Superbowl box score data.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/2014_super_bowl.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Find the number of turnovers the Seahawks committed.\n",
    "turnovers = parser.select(\"#turnovers\")[0]\n",
    "seahawks_turnovers = turnovers.select(\"td\")[1]\n",
    "seahawks_turnovers_count = seahawks_turnovers.text\n",
    "print(seahawks_turnovers_count)\n",
    "\n",
    "tplays = parser.select(\"#total-plays\")[0]\n",
    "patriots_tplays=tplays.select(\"td\")[2]\n",
    "tyards = parser.select(\"#total-yards\")[0]\n",
    "seahawks_tyards=tyards.select(\"td\")[1]\n",
    "\n",
    "patriots_total_plays_count=patriots_tplays.text\n",
    "\n",
    "seahawks_total_yards_count=seahawks_tyards.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* combine requests and beautifulSoup (parser.select use css like)\n",
    "* Scrapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
