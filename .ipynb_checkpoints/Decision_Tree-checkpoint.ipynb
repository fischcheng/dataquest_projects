{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree classification on 1994 census to predict if income >50k\n",
    "* Before we get started with decision trees, we need to convert the categorical variables in our data set to numeric variables.\n",
    "* The numbers aren't always compatible with other libraries like Scikit-learn, though, so it's easier to just do the conversion to numeric upfront.\n",
    "* Does this work the same as pd.get_dummies? \n",
    "\n",
    "### Notes\n",
    "* A decision tree is made up of a series of nodes and branches. A node is where we split the data based on a variable, and a branch is one side of the split. \n",
    "* When we do our splits, we aren't doing them randomly; we have an objective. Our goal is to ensure that we can make a prediction on future data\n",
    "\n",
    "### When to stop? aim for the target label only contains one category\n",
    "* In order to be able to make this prediction, all of the rows in a leaf should only have a single value\n",
    "* In order to do this, we need a metric for how \"together\" the different values in the high_income column are.\n",
    "* The \"Entropy\", The more \"mixed together\" 1s and 0s are, the higher the entropy. A data set consisting entirely of 1s in the high_income column would have low entropy\n",
    "* Information theory: A key concept in information theory is the notion of a bit of information. One bit of information is one unit of information.\n",
    "* Entropy can be much more complex, especially when we get to cases with more than two possible outcomes, or differential probabilities. \n",
    "* We get less than one \"bit\" of information -- only .97 -- because there are slightly more 1s in the sample data than 0s. This means that if we were predicting a new value, we could guess that the answer is 1 and be right more often than wrong (because there's a .6 probability of the answer being 1). Due to this prior knowledge, we gain less than a full \"bit\" of information when we observe a new value. >>> this read like Bayes\n",
    "\n",
    "\n",
    "*bit of information: like the one in Bayes Theorem book, how Turin built the bomb.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a single column from text categories to numbers\n",
    "col = pandas.Categorical(income[\"workclass\"])\n",
    "income[\"workclass\"] = col.codes\n",
    "print(income[\"workclass\"].head(5))\n",
    "#income[\"workclass\"].unique()\n",
    "\n",
    "for name in [\"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pandas.Categorical(income[name])\n",
    "    income[name] = col.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaparate the data into two halves: \n",
    "# Enter your code here  > in this case Public or Private sector\n",
    "private_incomes=income[income['workclass']==4]\n",
    "public_incomes=income[income['workclass']!=4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy:\n",
    "import math\n",
    "# We'll do the same calculation we did above, but in Python\n",
    "# Passing in 2 as the second parameter to math.log will take a base 2 log\n",
    "entropy = -(2/5 * math.log(2/5, 2) + 3/5 * math.log(3/5, 2))\n",
    "print(entropy)\n",
    "\n",
    "income['high_income'].value_counts()[0]\n",
    "\n",
    "ones=income['high_income'].value_counts()[1]\n",
    "zeros=income['high_income'].value_counts()[0]\n",
    "\n",
    "a=ones/(ones+zeros)\n",
    "b=zeros/(ones+zeros)\n",
    "\n",
    "\n",
    "income_entropy = -(a*math.log(a,2)+b*math.log(b,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which direction to iterate? Use what feature to separate?\n",
    "* Information gain, or which split can further reduce entropy?\n",
    "* To compute it, we first calculate the entropy for . Then, for each unique value  in the variable , we compute the number of rows in which  takes on the value , and divide it by the total number of rows. Next, we multiply the results by the entropy of the rows where  is . We add all of these subset entropies together, then subtract from the overall entropy to get information gain.\n",
    "* To simplify the calculation of information gain and make splits simpler, we won't do it for each unique value. We'll find the median for the variable we're splitting on instead. Any rows where the value of the variable is below the median will go to the left branch, and the rest of the rows will go to the right branch. To compute information gain, we'll only have to compute entropies for two subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def calc_entropy(column):\n",
    "    \"\"\"\n",
    "    Calculate entropy given a pandas series, list, or numpy array.\n",
    "    \"\"\"\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = numpy.bincount(column)\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    # Initialize the entropy to 0\n",
    "    entropy = 0\n",
    "    # Loop through the probabilities, and add each one to the total entropy\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    \n",
    "    return -entropy\n",
    "\n",
    "# Verify that our function matches our answer from earlier\n",
    "entropy = calc_entropy([1,1,0,0,1])\n",
    "print(entropy)\n",
    "\n",
    "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
    "print(information_gain)\n",
    "\n",
    "# Separate two branches\n",
    "median_age=income['age'].median()\n",
    "left_split= income[income['age']<=median_age]\n",
    "right_split=income[income['age']>median_age]\n",
    "\n",
    "a=len(left_split)/len(income)\n",
    "b=len(right_split)/len(income)\n",
    "\n",
    "\n",
    "age_information_gain= income_entropy - ((a * calc_entropy(left_split['high_income'])) + (b * calc_entropy(right_split['high_income'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each columns to find the one with the most information gain:\n",
    "def calc_information_gain(data, split_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate information gain given a data set, column to split on, and target\n",
    "    \"\"\"\n",
    "    # Calculate the original entropy\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    # Find the median of the column we're splitting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    # Make two subsets of the data, based on the median\n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    # Loop through the splits and calculate the subset entropies\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    # Return information gain\n",
    "    return original_entropy - to_subtract\n",
    "\n",
    "# Verify that our answer is the same as on the last screen\n",
    "print(calc_information_gain(income, \"age\", \"high_income\"))\n",
    "\n",
    "# My codes\n",
    "import numpy as np\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "\n",
    "information_gains=[]\n",
    "for col in columns:\n",
    "    information_gain=calc_information_gain(income, col, \"high_income\")\n",
    "    information_gains.append(information_gain)\n",
    "highest_gain=columns[np.argmax(information_gains)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a decision tree using recursive function\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_column(data, target_name, columns):\n",
    "    # Fill in the logic here to automatically find the column in columns to split on\n",
    "    # data is a dataframe\n",
    "    # target_name is the name of the target variable\n",
    "    # columns is a list of potential columns to split on\n",
    "    information_gains=[]\n",
    "    for col in columns:\n",
    "        information_gain=calc_information_gain(data, col, target_name)\n",
    "        information_gains.append(information_gain)\n",
    "    best=columns[information_gains.index(max(information_gains))]   \n",
    "    return best\n",
    "\n",
    "# A list of columns to potentially split income with\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "income_split=find_best_column(income,'high_income',columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use lists to store our labels for nodes (when we find them)\n",
    "# Lists can be accessed inside our recursive function, whereas integers can't.  \n",
    "# Look at the python missions on scoping for more information on this topic\n",
    "label_1s = []\n",
    "label_0s = []\n",
    "\n",
    "def id3(data, target, columns):\n",
    "    # The pandas.unique method will return a list of all the unique values in a series\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    \n",
    "    if len(unique_targets) == 1:\n",
    "        # Insert code here to append 1 to label_1s or 0 to label_0s, based on what we should label the node\n",
    "        # See lines 2 and 3 in the algorithm\n",
    "        if unique_targets==1:\n",
    "            label_1s.append(1)\n",
    "        else:\n",
    "            label_0s.append(0)\n",
    "        # Returning here is critical -- if we don't, the recursive tree will never finish, and run forever\n",
    "        # See our example above for when we returned\n",
    "        return\n",
    "    \n",
    "    # Find the best column to split on in our data\n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    # Find the median of the column\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    # Create the two splits\n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    \n",
    "    # Loop through the splits and call id3 recursively\n",
    "    for split in [left_split, right_split]:\n",
    "        # Call id3 recursively to process each branch\n",
    "        id3(split, target, columns)\n",
    "    \n",
    "# Create the data set that we used in the example on the last screen\n",
    "data = pandas.DataFrame([\n",
    "    [0,20,0],\n",
    "    [0,60,2],\n",
    "    [0,40,1],\n",
    "    [1,25,1],\n",
    "    [1,35,2],\n",
    "    [1,55,1]\n",
    "    ])\n",
    "# Assign column names to the data\n",
    "data.columns = [\"high_income\", \"age\", \"marital_status\"]\n",
    "\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use nested disctionary to store nodes info and label\n",
    "# Create a dictionary to hold the tree  \n",
    "# It has to be outside of the function so we can access it later\n",
    "tree = {}\n",
    "\n",
    "# This list will let us number the nodes  \n",
    "# It has to be a list so we can access it inside the function\n",
    "nodes = []\n",
    "\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    \n",
    "    # Assign the number key to the node dictionary\n",
    "    nodes.append(len(nodes) + 1) #start from 1, the root\n",
    "    tree[\"number\"] = nodes[-1]   #the # of that node\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        # Insert code here that assigns the \"label\" field to the node dictionary\n",
    "        if unique_targets==1:\n",
    "            tree['label']=1\n",
    "        else:\n",
    "            tree['label']=0\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    # Insert code here that assigns the \"column\" and \"median\" fields to the node dictionary\n",
    "    tree['column']=best_column\n",
    "    tree['median']=column_median\n",
    "    \n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"], tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out nested dictionary and its labels:\n",
    "def print_with_depth(string, depth):\n",
    "    # Add space before a string\n",
    "    prefix = \"    \" * depth\n",
    "    # Print a string, and indent it appropriately\n",
    "    print(\"{0}{1}\".format(prefix, string))\n",
    "    \n",
    "    \n",
    "def print_node(tree, depth):\n",
    "    # Check for the presence of \"label\" in the tree\n",
    "    if \"label\" in tree:\n",
    "        # If found, then this is a leaf, so print it and return\n",
    "        print_with_depth(\"Leaf: Label {0}\".format(tree[\"label\"]), depth)\n",
    "        # This is critical -- without it, you'll get infinite recursion\n",
    "        return\n",
    "    # Print information about what the node is splitting on\n",
    "    print_with_depth(\"{0} > {1}\".format(tree[\"column\"], tree[\"median\"]), depth)\n",
    "    \n",
    "    # Create a list of tree branches\n",
    "    branches = [tree[\"left\"], tree[\"right\"]]\n",
    "        \n",
    "    # Insert code here to recursively call print_node on each branch\n",
    "    for node in branches:\n",
    "        print_node(node,depth)\n",
    "    # Don't forget to increment depth when you pass it in\n",
    "    depth+=1\n",
    "    \n",
    "print_node(tree, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, row):\n",
    "    if \"label\" in tree:\n",
    "        return tree[\"label\"]\n",
    "    \n",
    "    column = tree[\"column\"]\n",
    "    median = tree[\"median\"]\n",
    "    \n",
    "    if row[column]<=median:\n",
    "        return predict(tree['left'],row)\n",
    "    else:\n",
    "        return predict(tree['right'],row)\n",
    "    \n",
    "    # Insert code here to check whether row[column] is less than or equal to median\n",
    "    # If it's less than or equal, return the result of predicting on the left branch of the tree\n",
    "    # If it's greater, return the result of predicting on the right branch of the tree\n",
    "    # Remember to use the return statement to return the result!\n",
    "\n",
    "# Print the prediction for the first row in our data\n",
    "print(predict(tree, data.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pandas.DataFrame([\n",
    "    [40,0],\n",
    "    [20,2],\n",
    "    [80,1],\n",
    "    [15,1],\n",
    "    [27,2],\n",
    "    [38,1]\n",
    "    ])\n",
    "# Assign column names to the data\n",
    "new_data.columns = [\"age\", \"marital_status\"]\n",
    "\n",
    "def batch_predict(tree, df):\n",
    "    # Insert your code here\n",
    "    df.apply(lambda x: predict(tree, x),axis=1)\n",
    "    pass\n",
    "\n",
    "predictions = batch_predict(tree, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* We can represent the root node with a dictionary, and branches with the keys left and right.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
