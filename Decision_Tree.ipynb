{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree classification on 1994 census to predict if income >50k\n",
    "* Before we get started with decision trees, we need to convert the categorical variables in our data set to numeric variables.\n",
    "* The numbers aren't always compatible with other libraries like Scikit-learn, though, so it's easier to just do the conversion to numeric upfront.\n",
    "* Does this work the same as pd.get_dummies? \n",
    "\n",
    "### Notes\n",
    "* A decision tree is made up of a series of nodes and branches. A node is where we split the data based on a variable, and a branch is one side of the split. \n",
    "* When we do our splits, we aren't doing them randomly; we have an objective. Our goal is to ensure that we can make a prediction on future data\n",
    "\n",
    "### When to stop? aim for the target label only contains one category\n",
    "* In order to be able to make this prediction, all of the rows in a leaf should only have a single value\n",
    "* In order to do this, we need a metric for how \"together\" the different values in the high_income column are.\n",
    "* The \"Entropy\", The more \"mixed together\" 1s and 0s are, the higher the entropy. A data set consisting entirely of 1s in the high_income column would have low entropy\n",
    "* Information theory: A key concept in information theory is the notion of a bit of information. One bit of information is one unit of information.\n",
    "* Entropy can be much more complex, especially when we get to cases with more than two possible outcomes, or differential probabilities. \n",
    "* We get less than one \"bit\" of information -- only .97 -- because there are slightly more 1s in the sample data than 0s. This means that if we were predicting a new value, we could guess that the answer is 1 and be right more often than wrong (because there's a .6 probability of the answer being 1). Due to this prior knowledge, we gain less than a full \"bit\" of information when we observe a new value. >>> this read like Bayes\n",
    "\n",
    "\n",
    "*bit of information: like the one in Bayes Theorem book, how Turin built the bomb.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a single column from text categories to numbers\n",
    "col = pandas.Categorical(income[\"workclass\"])\n",
    "income[\"workclass\"] = col.codes\n",
    "print(income[\"workclass\"].head(5))\n",
    "#income[\"workclass\"].unique()\n",
    "\n",
    "for name in [\"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pandas.Categorical(income[name])\n",
    "    income[name] = col.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaparate the data into two halves: \n",
    "# Enter your code here  > in this case Public or Private sector\n",
    "private_incomes=income[income['workclass']==4]\n",
    "public_incomes=income[income['workclass']!=4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy:\n",
    "import math\n",
    "# We'll do the same calculation we did above, but in Python\n",
    "# Passing in 2 as the second parameter to math.log will take a base 2 log\n",
    "entropy = -(2/5 * math.log(2/5, 2) + 3/5 * math.log(3/5, 2))\n",
    "print(entropy)\n",
    "\n",
    "income['high_income'].value_counts()[0]\n",
    "\n",
    "ones=income['high_income'].value_counts()[1]\n",
    "zeros=income['high_income'].value_counts()[0]\n",
    "\n",
    "a=ones/(ones+zeros)\n",
    "b=zeros/(ones+zeros)\n",
    "\n",
    "\n",
    "income_entropy = -(a*math.log(a,2)+b*math.log(b,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which direction to iterate? Use what feature to separate?\n",
    "* Information gain, or which split can further reduce entropy?\n",
    "* To compute it, we first calculate the entropy for . Then, for each unique value  in the variable , we compute the number of rows in which  takes on the value , and divide it by the total number of rows. Next, we multiply the results by the entropy of the rows where  is . We add all of these subset entropies together, then subtract from the overall entropy to get information gain.\n",
    "* To simplify the calculation of information gain and make splits simpler, we won't do it for each unique value. We'll find the median for the variable we're splitting on instead. Any rows where the value of the variable is below the median will go to the left branch, and the rest of the rows will go to the right branch. To compute information gain, we'll only have to compute entropies for two subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def calc_entropy(column):\n",
    "    \"\"\"\n",
    "    Calculate entropy given a pandas series, list, or numpy array.\n",
    "    \"\"\"\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = numpy.bincount(column)\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    # Initialize the entropy to 0\n",
    "    entropy = 0\n",
    "    # Loop through the probabilities, and add each one to the total entropy\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    \n",
    "    return -entropy\n",
    "\n",
    "# Verify that our function matches our answer from earlier\n",
    "entropy = calc_entropy([1,1,0,0,1])\n",
    "print(entropy)\n",
    "\n",
    "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
    "print(information_gain)\n",
    "\n",
    "# Separate two branches\n",
    "median_age=income['age'].median()\n",
    "left_split= income[income['age']<=median_age]\n",
    "right_split=income[income['age']>median_age]\n",
    "\n",
    "a=len(left_split)/len(income)\n",
    "b=len(right_split)/len(income)\n",
    "\n",
    "\n",
    "age_information_gain= income_entropy - ((a * calc_entropy(left_split['high_income'])) + (b * calc_entropy(right_split['high_income'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each columns to find the one with the most information gain:\n",
    "def calc_information_gain(data, split_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate information gain given a data set, column to split on, and target\n",
    "    \"\"\"\n",
    "    # Calculate the original entropy\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    # Find the median of the column we're splitting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    # Make two subsets of the data, based on the median\n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    # Loop through the splits and calculate the subset entropies\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    # Return information gain\n",
    "    return original_entropy - to_subtract\n",
    "\n",
    "# Verify that our answer is the same as on the last screen\n",
    "print(calc_information_gain(income, \"age\", \"high_income\"))\n",
    "\n",
    "# My codes\n",
    "import numpy as np\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "\n",
    "information_gains=[]\n",
    "for col in columns:\n",
    "    information_gain=calc_information_gain(income, col, \"high_income\")\n",
    "    information_gains.append(information_gain)\n",
    "highest_gain=columns[np.argmax(information_gains)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a decision tree using recursive function\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_column(data, target_name, columns):\n",
    "    # Fill in the logic here to automatically find the column in columns to split on\n",
    "    # data is a dataframe\n",
    "    # target_name is the name of the target variable\n",
    "    # columns is a list of potential columns to split on\n",
    "    information_gains=[]\n",
    "    for col in columns:\n",
    "        information_gain=calc_information_gain(data, col, target_name)\n",
    "        information_gains.append(information_gain)\n",
    "    best=columns[information_gains.index(max(information_gains))]   \n",
    "    return best\n",
    "\n",
    "# A list of columns to potentially split income with\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "income_split=find_best_column(income,'high_income',columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use lists to store our labels for nodes (when we find them)\n",
    "# Lists can be accessed inside our recursive function, whereas integers can't.  \n",
    "# Look at the python missions on scoping for more information on this topic\n",
    "label_1s = []\n",
    "label_0s = []\n",
    "\n",
    "def id3(data, target, columns):\n",
    "    # The pandas.unique method will return a list of all the unique values in a series\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    \n",
    "    if len(unique_targets) == 1:\n",
    "        # Insert code here to append 1 to label_1s or 0 to label_0s, based on what we should label the node\n",
    "        # See lines 2 and 3 in the algorithm\n",
    "        if unique_targets==1:\n",
    "            label_1s.append(1)\n",
    "        else:\n",
    "            label_0s.append(0)\n",
    "        # Returning here is critical -- if we don't, the recursive tree will never finish, and run forever\n",
    "        # See our example above for when we returned\n",
    "        return\n",
    "    \n",
    "    # Find the best column to split on in our data\n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    # Find the median of the column\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    # Create the two splits\n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    \n",
    "    # Loop through the splits and call id3 recursively\n",
    "    for split in [left_split, right_split]:\n",
    "        # Call id3 recursively to process each branch\n",
    "        id3(split, target, columns)\n",
    "    \n",
    "# Create the data set that we used in the example on the last screen\n",
    "data = pandas.DataFrame([\n",
    "    [0,20,0],\n",
    "    [0,60,2],\n",
    "    [0,40,1],\n",
    "    [1,25,1],\n",
    "    [1,35,2],\n",
    "    [1,55,1]\n",
    "    ])\n",
    "# Assign column names to the data\n",
    "data.columns = [\"high_income\", \"age\", \"marital_status\"]\n",
    "\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use nested disctionary to store nodes info and label\n",
    "# Create a dictionary to hold the tree  \n",
    "# It has to be outside of the function so we can access it later\n",
    "tree = {}\n",
    "\n",
    "# This list will let us number the nodes  \n",
    "# It has to be a list so we can access it inside the function\n",
    "nodes = []\n",
    "\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    \n",
    "    # Assign the number key to the node dictionary\n",
    "    nodes.append(len(nodes) + 1) #start from 1, the root\n",
    "    tree[\"number\"] = nodes[-1]   #the # of that node\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        # Insert code here that assigns the \"label\" field to the node dictionary\n",
    "        if unique_targets==1:\n",
    "            tree['label']=1\n",
    "        else:\n",
    "            tree['label']=0\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    # Insert code here that assigns the \"column\" and \"median\" fields to the node dictionary\n",
    "    tree['column']=best_column\n",
    "    tree['median']=column_median\n",
    "    \n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"], tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out nested dictionary and its labels:\n",
    "def print_with_depth(string, depth):\n",
    "    # Add space before a string\n",
    "    prefix = \"    \" * depth\n",
    "    # Print a string, and indent it appropriately\n",
    "    print(\"{0}{1}\".format(prefix, string))\n",
    "    \n",
    "    \n",
    "def print_node(tree, depth):\n",
    "    # Check for the presence of \"label\" in the tree\n",
    "    if \"label\" in tree:\n",
    "        # If found, then this is a leaf, so print it and return\n",
    "        print_with_depth(\"Leaf: Label {0}\".format(tree[\"label\"]), depth)\n",
    "        # This is critical -- without it, you'll get infinite recursion\n",
    "        return\n",
    "    # Print information about what the node is splitting on\n",
    "    print_with_depth(\"{0} > {1}\".format(tree[\"column\"], tree[\"median\"]), depth)\n",
    "    \n",
    "    # Create a list of tree branches\n",
    "    branches = [tree[\"left\"], tree[\"right\"]]\n",
    "        \n",
    "    # Insert code here to recursively call print_node on each branch\n",
    "    for node in branches:\n",
    "        print_node(node,depth)\n",
    "    # Don't forget to increment depth when you pass it in\n",
    "    depth+=1\n",
    "    \n",
    "print_node(tree, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, row):\n",
    "    if \"label\" in tree:\n",
    "        return tree[\"label\"]\n",
    "    \n",
    "    column = tree[\"column\"]\n",
    "    median = tree[\"median\"]\n",
    "    \n",
    "    if row[column]<=median:\n",
    "        return predict(tree['left'],row)\n",
    "    else:\n",
    "        return predict(tree['right'],row)\n",
    "    \n",
    "    # Insert code here to check whether row[column] is less than or equal to median\n",
    "    # If it's less than or equal, return the result of predicting on the left branch of the tree\n",
    "    # If it's greater, return the result of predicting on the right branch of the tree\n",
    "    # Remember to use the return statement to return the result!\n",
    "\n",
    "# Print the prediction for the first row in our data\n",
    "print(predict(tree, data.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pandas.DataFrame([\n",
    "    [40,0],\n",
    "    [20,2],\n",
    "    [80,1],\n",
    "    [15,1],\n",
    "    [27,2],\n",
    "    [38,1]\n",
    "    ])\n",
    "# Assign column names to the data\n",
    "new_data.columns = [\"age\", \"marital_status\"]\n",
    "\n",
    "def batch_predict(tree, df):\n",
    "    # Insert your code here\n",
    "    return df.apply(lambda x: predict(tree, x),axis=1)\n",
    "\n",
    "predictions = batch_predict(tree, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Scikit-learn to do it!\n",
    "* AUC ranges from 0 to 1, so it's ideal for binary classification. The higher the AUC, the more accurate our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# A list of columns to train with\n",
    "# We've already converted all columns to numeric\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "# Instantiate the classifier\n",
    "# Set random_state to 1 to make sure the results are consistent\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# We've already loaded the variable \"income,\" which contains all of the income data\n",
    "clf.fit(income[columns],income['high_income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "# Set a random seed so the shuffle is the same every time\n",
    "numpy.random.seed(1)\n",
    "\n",
    "# Shuffle the rows  \n",
    "# This permutes the index randomly using numpy.random.permutation\n",
    "# Then, it reindexes the dataframe with the result\n",
    "# The net effect is to put the rows into random order\n",
    "income = income.reindex(numpy.random.permutation(income.index))\n",
    "\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "\n",
    "train=income.iloc[:train_max_row]\n",
    "test=income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AUC as the metric to evaluate the performance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "error=roc_auc_score(test['high_income'],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the model by tweaking parameters to avoid overfitting\n",
    "clf = DecisionTreeClassifier(random_state=1,min_samples_split=13)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc=roc_auc_score(test['high_income'],predictions)\n",
    "print(test_auc)\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "train_auc=roc_auc_score(train['high_income'],predictions)\n",
    "print(train_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment on different parameters\n",
    "clf = DecisionTreeClassifier(random_state=1,min_samples_split=13, max_depth=7)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc=roc_auc_score(test['high_income'],predictions)\n",
    "print(test_auc)\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "train_auc=roc_auc_score(train['high_income'],predictions)\n",
    "print(train_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random noise to test high-variance: difference \n",
    "# between test and train will increase because of overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* We can represent the root node with a dictionary, and branches with the keys left and right.\n",
    "* scikit-learn: sklearn.tree >> DecisionTreeClassifier and DecisionTreeRegressor\n",
    "* for roc_auc_score > the true/predictions should be in order\n",
    "* Splitting the data into training and testing sets doesn't prevent overfitting -- it just helps us detect and fix it.\n",
    "* This may seem to be a strange principle at first, but the deeper a tree is, the worse it typically performs on new data.\n",
    "\n",
    "#### Three ways to avoid overfitting\n",
    "* Prune the tree after building one\n",
    "* Use ensembling to blend the predictions of many trees\n",
    "* Restrict the depth of the tree\n",
    "\n",
    "#### Verdict\n",
    "* By artificially restricting the depth of our tree, we prevent it from creating a model that's complex enough to correctly categorize some of the rows. If we don't perform the artificial restrictions, however, the tree becomes too complex, fits quirks in the data that only exist in the training set, and doesn't generalize to new data.\n",
    "\n",
    "#### Advantage of Decision Tree\n",
    "* Easy to interpret\n",
    "* Relatively fast to fit and make predictions\n",
    "* Able to handle multiple types of data\n",
    "* Able to pick up nonlinearities in data, and usually fairly accurate\n",
    "\n",
    "### Disadvantage:\n",
    "* Tendency to overfit >>> solution: random forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Forest\n",
    "* A random forest is an ensemble of decision trees\n",
    "* fit the data with models with different parameters, features, then come up with a final predictions\n",
    "* Could be based on majority vote, or simply rounded mean.\n",
    "* use predict_proba instead (and take the mean across models)\n",
    "\n",
    "#### Two main ways\n",
    "* bagging (Bootstrap Aggregation): Some rows from the original data may appear in the \"bag\" multiple times.\n",
    "* random feature subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two simple models, and their auc scores respectively\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=5)\n",
    "clf2.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions1=clf.predict(test[columns])\n",
    "predictions2=clf2.predict(test[columns])\n",
    "\n",
    "auc1=roc_auc_score(test['high_income'],predictions1)\n",
    "auc2=roc_auc_score(test['high_income'],predictions2)\n",
    "\n",
    "print(\"auc1: \",auc1, \"auc2:\", auc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of proba across two models\n",
    "predictions = clf.predict_proba(test[columns])[:,1]\n",
    "predictions2 = clf2.predict_proba(test[columns])[:,1]\n",
    "\n",
    "final=numpy.round((predictions+predictions2)/2)\n",
    "\n",
    "print(roc_auc_score(test['high_income'],final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement  \n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample in every loop\n",
    "    # That would make all of our trees the same\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "\n",
    "final=numpy.round(numpy.mean(predictions,axis=0))\n",
    "\n",
    "print(roc_auc_score(test['high_income'],final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Scikit-learn\n",
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement\n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample every time\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2,splitter='random',max_features='auto')\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "\n",
    "combined = numpy.sum(predictions, axis=0) / 10\n",
    "rounded = numpy.round(combined)\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use RandomForestClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "predictions=clf.predict(test[columns])\n",
    "roc_auc_score(test['high_income'],predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* The more \"diverse\" or dissimilar the models we use to construct an ensemble are, the stronger their combined predictions will be (assuming that all of the models have about the same accuracy).\n",
    "* Weighted ensemble\n",
    "\n",
    "#### Scikitlearn\n",
    "There are also parameters specific to the random forest that alter its overall construction:\n",
    "\n",
    "n_estimators\n",
    "bootstrap - \"Bootstrap aggregation\" is another name for bagging; this parameter indicates whether to turn it on (Defaults to True)\n",
    "\n",
    "\n",
    "#### Advantage:\n",
    "* Very accurate predictions - Random forests achieve near state-of-the-art performance on many machine learning tasks. Along with neural networks and gradient-boosted trees, they're typically one of the top-performing algorithms.\n",
    "* Resistance to overfitting - Due to their construction, random forests are fairly resistant to overfitting. We still need to set and tweak parameters like max_depth though.\n",
    "\n",
    "#### Weakness:\n",
    "* They're difficult to interpret - Because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "* They take longer to create - Making two trees takes twice as long as making one, making three takes three times as long, and so on. Fortunately, we can exploit multicore processors to parallelize tree construction. Scikit allows us to do this through the n_jobs parameter on RandomForestClassifier. We'll discuss parallelization in greater detail later on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
